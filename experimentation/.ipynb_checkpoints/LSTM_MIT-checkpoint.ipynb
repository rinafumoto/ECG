{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f78cdfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                16896     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 25,861\n",
      "Trainable params: 25,861\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.1421 - accuracy: 0.5385\n",
      "Epoch 00001: loss improved from inf to 1.14209, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 177s 113ms/step - loss: 1.1421 - accuracy: 0.5385 - val_loss: 1.1036 - val_accuracy: 0.5790\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.9012 - accuracy: 0.6693\n",
      "Epoch 00002: loss improved from 1.14209 to 0.90118, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 179s 114ms/step - loss: 0.9012 - accuracy: 0.6693 - val_loss: 1.0429 - val_accuracy: 0.6140\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.9814 - accuracy: 0.6282\n",
      "Epoch 00003: loss did not improve from 0.90118\n",
      "1563/1563 [==============================] - 183s 117ms/step - loss: 0.9814 - accuracy: 0.6282 - val_loss: 0.9643 - val_accuracy: 0.6909\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7628 - accuracy: 0.7235\n",
      "Epoch 00004: loss improved from 0.90118 to 0.76285, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 183s 117ms/step - loss: 0.7628 - accuracy: 0.7235 - val_loss: 0.7852 - val_accuracy: 0.7759\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6811 - accuracy: 0.7549\n",
      "Epoch 00005: loss improved from 0.76285 to 0.68109, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 181s 116ms/step - loss: 0.6811 - accuracy: 0.7549 - val_loss: 0.8552 - val_accuracy: 0.7268\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6307 - accuracy: 0.7721\n",
      "Epoch 00006: loss improved from 0.68109 to 0.63066, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 173s 111ms/step - loss: 0.6307 - accuracy: 0.7721 - val_loss: 0.6300 - val_accuracy: 0.8137\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.7862\n",
      "Epoch 00007: loss improved from 0.63066 to 0.59088, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 181s 116ms/step - loss: 0.5909 - accuracy: 0.7862 - val_loss: 0.9081 - val_accuracy: 0.6729\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.7958\n",
      "Epoch 00008: loss improved from 0.59088 to 0.56436, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 189s 121ms/step - loss: 0.5644 - accuracy: 0.7958 - val_loss: 0.8541 - val_accuracy: 0.7150\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5136 - accuracy: 0.8137\n",
      "Epoch 00009: loss improved from 0.56436 to 0.51361, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 190s 122ms/step - loss: 0.5136 - accuracy: 0.8137 - val_loss: 0.6215 - val_accuracy: 0.7849\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.8224\n",
      "Epoch 00010: loss improved from 0.51361 to 0.49298, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 192s 123ms/step - loss: 0.4930 - accuracy: 0.8224 - val_loss: 0.6226 - val_accuracy: 0.8057\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4725 - accuracy: 0.8304\n",
      "Epoch 00011: loss improved from 0.49298 to 0.47251, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 188s 121ms/step - loss: 0.4725 - accuracy: 0.8304 - val_loss: 0.5700 - val_accuracy: 0.8553\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4784 - accuracy: 0.8283\n",
      "Epoch 00012: loss did not improve from 0.47251\n",
      "1563/1563 [==============================] - 190s 121ms/step - loss: 0.4784 - accuracy: 0.8283 - val_loss: 0.5593 - val_accuracy: 0.8218\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.8434\n",
      "Epoch 00013: loss improved from 0.47251 to 0.43305, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 189s 121ms/step - loss: 0.4331 - accuracy: 0.8434 - val_loss: 0.5048 - val_accuracy: 0.8302\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.8371\n",
      "Epoch 00014: loss did not improve from 0.43305\n",
      "1563/1563 [==============================] - 189s 121ms/step - loss: 0.4491 - accuracy: 0.8371 - val_loss: 0.5912 - val_accuracy: 0.7855\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4549 - accuracy: 0.8359\n",
      "Epoch 00015: loss did not improve from 0.43305\n",
      "1563/1563 [==============================] - 187s 120ms/step - loss: 0.4549 - accuracy: 0.8359 - val_loss: 0.5719 - val_accuracy: 0.8006\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.3988 - accuracy: 0.8561\n",
      "Epoch 00016: loss improved from 0.43305 to 0.39884, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 190s 121ms/step - loss: 0.3988 - accuracy: 0.8561 - val_loss: 0.4300 - val_accuracy: 0.8769\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.8613\n",
      "Epoch 00017: loss improved from 0.39884 to 0.38222, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 189s 121ms/step - loss: 0.3822 - accuracy: 0.8613 - val_loss: 0.5905 - val_accuracy: 0.7968\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8687\n",
      "Epoch 00018: loss improved from 0.38222 to 0.36386, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 188s 120ms/step - loss: 0.3639 - accuracy: 0.8687 - val_loss: 0.4543 - val_accuracy: 0.8563\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.3545 - accuracy: 0.8715\n",
      "Epoch 00019: loss improved from 0.36386 to 0.35447, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 188s 120ms/step - loss: 0.3545 - accuracy: 0.8715 - val_loss: 0.5878 - val_accuracy: 0.7786\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.8761\n",
      "Epoch 00020: loss improved from 0.35447 to 0.34614, saving model to weights_lstm.hdf5\n",
      "1563/1563 [==============================] - 189s 121ms/step - loss: 0.3461 - accuracy: 0.8761 - val_loss: 0.5011 - val_accuracy: 0.8086\n",
      "343/343 [==============================] - 17s 50ms/step - loss: 0.5011 - accuracy: 0.8086\n",
      "0.5011343955993652 0.8085601925849915\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('../data/mitbih_train.csv', header=None)\n",
    "test_df = pd.read_csv('../data/mitbih_test.csv', header=None)\n",
    "weight_path = \"weights_lstm.hdf5\"\n",
    "\n",
    "df_1 = train_df[train_df[187] == 1]\n",
    "df_2 = train_df[train_df[187] == 2]\n",
    "df_3 = train_df[train_df[187] == 3]\n",
    "df_4 = train_df[train_df[187] == 4]\n",
    "df_0 = (train_df[train_df[187] == 0]).sample(n=20000, random_state=42)\n",
    "\n",
    "df_1_upsample = resample(df_1, replace=True, n_samples=20000, random_state=123)\n",
    "df_2_upsample = resample(df_2, replace=True, n_samples=20000, random_state=124)\n",
    "df_3_upsample = resample(df_3, replace=True, n_samples=20000, random_state=125)\n",
    "df_4_upsample = resample(df_4, replace=True, n_samples=20000, random_state=126)\n",
    "\n",
    "train_df = pd.concat([df_0, df_1_upsample, df_2_upsample, df_3_upsample, df_4_upsample])\n",
    "\n",
    "y_train = to_categorical(train_df[187], 5)\n",
    "y_test = to_categorical(test_df[187], 5)\n",
    "\n",
    "x_train = train_df.iloc[:, :187].values\n",
    "x_test = test_df.iloc[:, :187].values\n",
    "x_train = x_train.reshape(len(x_train), x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(len(x_test), x_test.shape[1], 1)\n",
    "\n",
    "train_weights = True\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(187, 1)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "if train_weights:\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[checkpoint])\n",
    "\n",
    "else:\n",
    "    model.load_weights(weight_path)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(score, acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
